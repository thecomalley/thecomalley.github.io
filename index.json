[{"categories":null,"contents":"In this post, we’ll explore how to replicate GitHub Copilot’s pull request (PR) description functionality within Azure DevOps, using a custom solution I’ve called DescriptoBot. The goal is to automate the creation of PR descriptions by leveraging Azure OpenAI models, providing a streamlined, AI-driven method to summarize changes and improve efficiency across teams.\nThe Problem: Manual PR Descriptions Writing clear and concise PR descriptions can be a time-consuming task. Developers often need to summarize changes made in a feature branch, highlight key modifications, and provide context, which can easily be overlooked in fast-paced environments. While GitHub Copilot provides a useful feature for generating PR descriptions automatically, Azure DevOps lacks this native functionality.\nTo solve this, I built DescriptoBot, a simple python script that can be manually triggered by a developer when creating a PR in Azure DevOps. The script works by\u0026hellip;\nFetching the diff between the source and target branches. Passing the diff summary and full diff to OpenAI to generate a meaningful PR description. Suggesting a PR title based on the changes. Calculating the cost of the OpenAI prompt and output tokens. Automatically updating the PR description in Azure DevOps. Example Output Core Python Code Let\u0026rsquo;s break down some of the key components of the DescriptoBot script.\nFetching the Git Diff Here we simply use the git diff command to get the difference between the source and target branches, both in summary and full form. This provides the context for the OpenAI model to generate a meaningful PR description.\ndef get_diff(source_branch, target_branch): \u0026#34;\u0026#34;\u0026#34; Get the diff between the source and target branches. Parameters: source_branch (str): The name of the source branch. target_branch (str): The name of the target branch. Returns: tuple: The diff summary and the full diff. Raises: Exception: If there was an error getting the diff. \u0026#34;\u0026#34;\u0026#34; logging.info(f\u0026#34;Getting diff between {source_branch} and {target_branch}\u0026#34;) try: diff_summary = subprocess.check_output([\u0026#34;git\u0026#34;, \u0026#34;diff\u0026#34;, \u0026#34;--compact-summary\u0026#34;, target_branch, source_branch]).decode(\u0026#34;utf-8\u0026#34;) diff = subprocess.check_output([\u0026#34;git\u0026#34;, \u0026#34;diff\u0026#34;, target_branch, source_branch]).decode(\u0026#34;utf-8\u0026#34;) except Exception as e: logging.error(f\u0026#34;Failed to get diff: {e}\u0026#34;) raise else: return diff_summary, diff Tokenizing the Diff Using the tiktoken library, we encode the diff into tokens, which helps us calculate the cost of using the OpenAI model based on the number of tokens.\nimport tiktoken enc = tiktoken.encoding_for_model(model[\u0026#39;name\u0026#39;]) tokens = enc.encode(diff) diff_cost = len(tokens) * model[\u0026#34;input_cost\u0026#34;] print(f\u0026#34;Git diff tokens: {len(tokens)}, Cost: ${diff_cost:.2f}\u0026#34;) OpenAI client = AzureOpenAI( azure_endpoint=os.getenv(\u0026#34;AZURE_OPENAI_ENDPOINT\u0026#34;), api_key=os.getenv(\u0026#34;AZURE_OPENAI_API_KEY\u0026#34;), api_version=\u0026#34;2024-02-01\u0026#34; ) system_message = f\u0026#34;\u0026#34;\u0026#34; You are a bot that will help create an Azure DevOps Pull Request description based on a git diff summary and full diff. Include a Summary and Key Changes section only Suggest a PR title based on the changes using an gitmoji prefix this should be the first line of the PR description, don\u0026#39;t prefix with # or ##. There is a character limit of 4000 characters for the PR description so keep it below that. \u0026#34;\u0026#34;\u0026#34; response = client.chat.completions.create( model=\u0026#34;gpt-4o\u0026#34;, # model = \u0026#34;deployment_name\u0026#34;. messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: system_message}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Create a Pull Request description based on the git diff.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: diff_summary}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: diff} ] ) ","date":"October 16, 2024","hero":"/images/default-hero.jpg","permalink":"https://thecomalley.github.io/posts/descriptobot/","summary":"\u003cp\u003eIn this post, we’ll explore how to replicate \u003ca href=\"https://docs.github.com/en/enterprise-cloud@latest/copilot/using-github-copilot/creating-a-pull-request-summary-with-github-copilot\" target=\"_blank\" rel=\"noopener\"\u003eGitHub Copilot’s pull request (PR) description functionality\u003c/a\u003e within Azure DevOps, using a custom solution I’ve called DescriptoBot. The goal is to automate the creation of PR descriptions by leveraging Azure OpenAI models, providing a streamlined, AI-driven method to summarize changes and improve efficiency across teams.\u003c/p\u003e\n\u003ch3 id=\"the-problem-manual-pr-descriptions\"\u003eThe Problem: Manual PR Descriptions\u003c/h3\u003e\n\u003cp\u003eWriting clear and concise PR descriptions can be a time-consuming task. Developers often need to summarize changes made in a feature branch, highlight key modifications, and provide context, which can easily be overlooked in fast-paced environments. While GitHub Copilot provides a useful feature for generating PR descriptions automatically, Azure DevOps lacks this native functionality.\u003c/p\u003e","tags":null,"title":"Automating Pull Request Descriptions in Azure DevOps"},{"categories":["Git"],"contents":"Greetings! This post will guide you on how to configure Git to use different author email addresses based on where you clone the repository in your local directory. This can be particularly useful if you have different email addresses for personal projects, work, and different clients.\nHere\u0026rsquo;s an example of how you might organize your directories:\ncode/personal: For personal projects, you might want to use your personal email address. code/work: For your job, you might want to use your work email address. code/client1: For your first client, you might want to use a specific client email address. code/client2: For your second client, you might want to use a different client email address. To achieve this, you can use the includeIf directive in your global ~/.gitconfig file:\n[includeIf \u0026#34;gitdir:~/code/personal/\u0026#34;] path = .gitconfig-personal [includeIf \u0026#34;gitdir:~/code/work/\u0026#34;] path = .gitconfig-work [includeIf \u0026#34;gitdir:~/code/client1/\u0026#34;] path = .gitconfig-client1 [includeIf \u0026#34;gitdir:~/code/client2/\u0026#34;] path = .gitconfig-client2 In this configuration, you would have four separate .gitconfig files: .gitconfig-personal, .gitconfig-work, .gitconfig-client1, and .gitconfig-client2. Each of these files would contain the user email configuration for the respective directory.\nFor example, the .gitconfig-personal file might look like this:\n[user] email = personal-email@domain.com And the .gitconfig-work file might look like this:\n[user] email = work-email@domain.com And so on for client1 and client2. Remember to replace personal-email@domain.com, work-email@domain.com, etc., with the actual email addresses you want to use.\nTo validate that your configuration is working as expected, you can use the git config user.email command in the terminal while inside the repository directory. This command will display the email address that Git is currently configured to use for that repository. If the output matches the email address you specified in the .gitconfig file for that directory, then you\u0026rsquo;ve successfully configured Git to use different author email addresses based on the repository location.\nHappy coding!\n","date":"May 27, 2024","hero":"/posts/git-emails/hero.png","permalink":"https://thecomalley.github.io/posts/git-emails/","summary":"\u003cp\u003eGreetings! This post will guide you on how to configure Git to use different author email addresses based on where you clone the repository in your local directory. This can be particularly useful if you have different email addresses for personal projects, work, and different clients.\u003c/p\u003e\n\u003cp\u003eHere\u0026rsquo;s an example of how you might organize your directories:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ecode/personal\u003c/code\u003e: For personal projects, you might want to use your personal email address.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecode/work\u003c/code\u003e: For your job, you might want to use your work email address.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecode/client1\u003c/code\u003e: For your first client, you might want to use a specific client email address.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecode/client2\u003c/code\u003e: For your second client, you might want to use a different client email address.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo achieve this, you can use the \u003ccode\u003eincludeIf\u003c/code\u003e directive in your global \u003ccode\u003e~/.gitconfig\u003c/code\u003e file:\u003c/p\u003e","tags":["Git","Configuration","Multi-Email"],"title":"Configuring Git to Use Different Author Email Addresses Based on Repository Location"},{"categories":["Basic"],"contents":"Terraform, Azure, Ansible \u0026amp; Windows Config management has been something on the back of my mind to dive into but have never quite got around to it, so its about time to have a look at ansible! The is to to provision \u0026amp; configure a Windows VM without having to touch a GUI\n1. Deploy the VM Firstly we need to deploy a VM to Azure, for this i\u0026rsquo;m just using the example code provided with the azurerm_windows_virtual_machine with the addition of a Public IP to manage remotely,\nBefore running apply i make sure to check the awesome Azure VM Comparison site to see where the cheapest VM is, (gotta stretch that MSDN budget!!)\n2 Create an inventory file Now we have our VM up we need to connect to it, To do this we need to generate an inventory similar to below\nwindows: hosts: my.public.ip: ansible_user: vm-username ansible_password: vm-password We Copy paste this info from the portal and the terraform code, but what if we are using random generators for the passwords or are just lazy? if only there was a way to pipe terraform outputs into an ansible inventory file\u0026hellip;\nEnter terraform templatefile \u0026amp; local_file\nBasically the template function reads a file and updates it with a supplied set of variables The local_file resource then writes the updated content back to a file on the drive that ansible can reference!\nansible.tf\nresource \u0026#34;local_file\u0026#34; \u0026#34;ansible\u0026#34; { content = templatefile(\u0026#34;${path.module}/ansible.tpl\u0026#34;, { hosts = azurerm_public_ip.example.ip_address ansible_user = azurerm_windows_virtual_machine.example.admin_username ansible_password = azurerm_windows_virtual_machine.example.admin_password } ) filename = \u0026#34;../ansible/inventory.yml\u0026#34; depends_on = [ azurerm_windows_virtual_machine.example azurerm_public_ip.example ] } ansible.tpl\nwindows: hosts: ${hosts}: ansible_user: ${ansible_user} ansible_password: ${ansible_password} vars: ansible_connection: winrm ansible_winrm_server_cert_validation: ignore 3. Enable WinRM Great we now have a windows vm and an inventory file generated all via terraform!\nYou could try accessing the vm via ansible -i 'inventory.yml' windows -m win_ping but i doubt you\u0026rsquo;d have much luck. We need to enable WinRM on the server first! luckily the folks over at ansible have a great powershell script for doing this.\nWe now just need to deploy it to the server somehow\u0026hellip;\nThanks to azurerm_virtual_machine_extension we can easily run a powershell script on the vm when its deployed!\nresource \u0026#34;azurerm_virtual_machine_extension\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;ConfigureRemotingForAnsible\u0026#34; virtual_machine_id = azurerm_windows_virtual_machine.example.id publisher = \u0026#34;Microsoft.Compute\u0026#34; type = \u0026#34;CustomScriptExtension\u0026#34; type_handler_version = \u0026#34;1.9\u0026#34; settings = \u0026lt;\u0026lt;SETTINGS { \u0026#34;fileUris\u0026#34;:[\u0026#34;https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1\u0026#34;], \u0026#34;commandToExecute\u0026#34;: \u0026#34;powershell.exe -Command \\\u0026#34;./ConfigureRemotingForAnsible.ps1; exit 0;\\\u0026#34;\u0026#34; } SETTINGS } Run Ansible! Now we have WinRM setup on the VM we can access it via ansible using the command from earlier inventory.yml' windows -m win_ping awesome! but pinging a VM isn\u0026rsquo;t much fun lets make a playbook!\nI\u0026rsquo;m still getting up to speed with how Ansible works for our / my fist playbook all we need to know is Ansible has\nplugins (kinda like terraform resources) collections (kinda like terraform providers) So we can grab the ansible.windows.win_feature plugin from the Ansible.Windows collection and add it to a playbook like below!\nplaybook.yml\n--- - name: My First Playbook hosts: windows tasks: - name: Install AD-Domain-Services with sub features and management tools ansible.windows.win_feature: name: AD-Domain-Services state: present include_sub_features: yes include_management_tools: yes now to run the whole thing we just run ansible-playbook -i 'inventory.yml' playbook.yml Awesome!\nNext Steps So there\u0026rsquo;s probably heaps of things i\u0026rsquo;ve missed or could do better\nHow to pass vars securely from Terraform to Ansible, (Ansible Vault? KeyVault?) What about Ansible AWX / Tower / Ansible Automation Platform?? What does an end to end VM config playbook look like? Sounds like a blog for another day\n","date":"October 13, 2021","hero":"/posts/ansible/ansible.jpg","permalink":"https://thecomalley.github.io/posts/ansible/","summary":"\u003ch1 id=\"terraform-azure-ansible--windows\"\u003eTerraform, Azure, Ansible \u0026amp; Windows\u003c/h1\u003e\n\u003cp\u003eConfig management has been something on the back of my mind to dive into but have never quite got around to it, so its about time to have a look at ansible!\nThe is to to provision \u0026amp; configure a Windows VM without having to touch a GUI\u003c/p\u003e\n\u003ch2 id=\"1-deploy-the-vm\"\u003e1. Deploy the VM\u003c/h2\u003e\n\u003cp\u003eFirstly we need to deploy a VM to Azure, for this i\u0026rsquo;m just using the example code provided with the \u003ca href=\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/windows_virtual_machine\" target=\"_blank\" rel=\"noopener\"\u003eazurerm_windows_virtual_machine\u003c/a\u003e with the addition of a Public IP to manage remotely,\u003c/p\u003e","tags":["Basic","Multi-lingual"],"title":"Ansible"},{"categories":null,"contents":"The electricity provider i\u0026rsquo;m with offers a changeable free \u0026ldquo;Hour Of Power\u0026rdquo; every day (off peak hours) So we set about making the most of this in our household, running the dishwasher washing machine and hot water all during this hour. But after a while we fell out of the habit, adjusting our lifestyle to fit this hour wasn\u0026rsquo;t really working\u0026hellip; Luckily the Electricity provider does provide the ability to change the Hour of Power for the current day (up to 11:59pm each day)\nWe can break the problem of automating this down into three parts Part 1 - Collecting realtime power consumption data Part 2 - Analysing data to find peak usage \u0026amp; updating the hour of power\nPart 1. Collecting realtime power consumption data The first real problem is access to data. despite the so called smart meters its really hard to get access to your own data! now the electricity company\u0026rsquo;s obviously have this and most are pretty good about displaying it back to the user via their respective apps, but nearly all suffer a delay of at lest a day\u0026hellip; that\u0026rsquo;s not gonna cut it for us since we need to run a calculation before midnight each day.\nAfter a some advice from a colleague i picked up an efergy Engage Hub Kit second hand from TradeMe, as it has an integration for my favourite Home Automation Platform Home Assistant however when setting it up i was unable to register the hub. It looks to still be tied to the previous owners account. Efergy support has been unresponsive as well so not of to a great start!\nAfter a bit of googling i stumbled across the awesome opensource project rtl_433 it just so happens that the Effergy transmitters have been decoded by the project! so after a quick visit back to Trademe (not patient enough for an Ali order by this point) i picked up a cheap Software Defined Radio\nPulling out the Raspberry Pi i installed rtl_433 using this guide and started scanning for messages\u0026hellip;\nBingo! there it was the data right from the transmitter no need for a engage hub\nIngesting the Data Now we needed to get the data into Home Assistant so i could use the awesome new Energy Management Feature.\nI found a Home assistant addon (Docker Container) for running rtl_433 that sends the output from rtl_433 to a MQTT\nI already have MQTT server running so i spun up the container added a config file rtl_433.conf and ran rtl_433 again frequency \u0026amp; protocol\noutput mqtt://HOST:PORT,user=XXXX,pass=YYYYYYY frequency 433.485M protocol 36 convert si report_meta newmodel Image of MQTT explorer Now the data was being sent to MQTT we needed to get it into Home Assistant\nHere we are defining a sensor looking at the current / Amps that the Transmitter is reporting, but really we need kWh so we need to do a bit more work\nsensor: - platform: mqtt name: \u0026#34;House Current\u0026#34; device_class: \u0026#34;current\u0026#34; unit_of_measurement: \u0026#34;A\u0026#34; state_topic: \u0026#34;rtl_433/9b13b3f4-rtl433/devices/Efergy-e2CT/34737/current\u0026#34; Now faced with a Year 10 Physics lesson we needed to convert the Amps into Watts. we do this by multiplying by 240. Its not completely accurate because the voltage isn\u0026rsquo;t always a constant 240v but its good enough for now\ntemplate: - sensor: - name: \u0026#34;House Power\u0026#34; unit_of_measurement: \u0026#34;W\u0026#34; device_class: \u0026#34;power\u0026#34; state: \u0026#34;{{ states(\u0026#39;sensor.house_current\u0026#39;) | float * 240 }}\u0026#34; Finally using the confusingly named integration integration\nsensor: - platform: integration source: sensor.house_power name: \u0026#34;House Energy\u0026#34; unit_prefix: k round: 2 ","date":"October 13, 2021","hero":"/images/default-hero.jpg","permalink":"https://thecomalley.github.io/posts/electrickiwi/part1/","summary":"\u003cp\u003eThe electricity provider i\u0026rsquo;m with offers a changeable free \u0026ldquo;Hour Of Power\u0026rdquo; every day (off peak hours) So we set about making the most of this in our household, running the dishwasher washing machine and hot water all during this hour. But after a while we fell out of the habit, adjusting our lifestyle to fit this hour wasn\u0026rsquo;t really working\u0026hellip; Luckily the Electricity provider does provide the ability to change the Hour of Power for the current day (up to 11:59pm each day)\u003c/p\u003e","tags":null,"title":"Monitoring Electricity usage to save $$ - Part 1"},{"categories":null,"contents":"This is a quick and simple post about using monitoring a cron job with healthchecks.io in this case the script is a rsync job from an local server (unRAID) to azure blob but principles are pretty universal. you can checkout the code for this on github homelab-remote-backup\nhealthchecks.io provides \u0026ldquo;Simple and Effective Cron Job Monitoring\u0026rdquo; via hitting a http url, it can be used as a SaaS tool and is also open-source so you can self host it\nTerraform \u0026amp; healthchecks.io healthchecks.io is available as a terraform provider although to use the provider you must create a project and to get an API key specific to that healthchecks project.\nTerraform will provision a check and resulting API endpoint we need to hit for monitoring, but we want to take it a step further and add this endpoint to a bash script automatically. We can do this by using the terraform template_dir resource\n{% highlight terraform %} resource \u0026ldquo;template_dir\u0026rdquo; \u0026ldquo;config\u0026rdquo; { source_dir = \u0026ldquo;../rclone/templates\u0026rdquo; destination_dir = \u0026ldquo;../rclone/user_scripts\u0026rdquo;\nvars = { ping_url = healthchecksio_check.appdata.ping_url storage_account = azurerm_storage_account.example.name primary_access_key = azurerm_storage_account.example.primary_access_key } } {% endhighlight %}\nthis resource allows us to populate vars in .tpl files from terraform variables, outputs or other attributes. this keeps our potentially sensitive ping URL out of source control \u0026amp; ensures anytime changes are made to terraform the bash script is up to date.\nMonitoring cronjob Using healthchecks.io we are able to monitor three key aspects of our cronjob\nruntime of the script exit code from rclone job the logs from the rclone command start time of the script By adding /start to the end of the url we can tell healthchecks.io when the cronjob has started, obviously add this line near the top of the script. hitting the endpoint a second time without the /start will tell healthchecks.io the job has completed allowing us to record runtime.\n{% highlight bash %} curl -m 10 \u0026ndash;retry 5 $PING_URL/start {% endhighlight %}\nexit code from rclone job We can also send the exit code provided by our rclone command, to do this we store the exit code in the variable exit_code we do this by using the bash Special Parameter $? Finally by adding the exit code at the end of the url this allows us to pass the code to healthchecks.io\n{% highlight bash %} rclone command exit_code=$? curl -m 10 \u0026ndash;retry 5 $PING_URL/$exit_code {% endhighlight %}\nthe logs from the rclone command Finally we can attach the logs from the rclone command by passing them in the body --data-raw \u0026quot;$cmd\u0026quot; But first we need to capture the output (both the stdout and stderr streams) to do this we add -v 2\u0026gt;\u0026amp;1 to the end of the command\n-v is a flag for rsync that \u0026ldquo;Prints lots more stuff\u0026rdquo; 2\u0026gt;\u0026amp;1 will redirect stderr to whatever value is set to stdout, and we already piped stdout to the var cmd {% highlight bash %} cmd=$(rclone copy /mnt/user/Backup/appdata azure-remote-backup:appdata -v 2\u0026gt;\u0026amp;1) curl -m 10 \u0026ndash;retry 5 \u0026ndash;data-raw \u0026ldquo;$cmd\u0026rdquo; $PING_URL {% endhighlight %}\nCheck the repo for what this looks like all combined!\n","date":"June 30, 2021","hero":"/posts/cron/healthchecks.jpg","permalink":"https://thecomalley.github.io/posts/cron/","summary":"\u003cp\u003eThis is a quick and simple post about using monitoring a cron job with \u003ca href=\"https://healthchecks.io/\" target=\"_blank\" rel=\"noopener\"\u003ehealthchecks.io\u003c/a\u003e\nin this case the script is a rsync job from an local server (unRAID) to azure blob but principles are pretty universal.\nyou can checkout the code for this on github \u003ca href=\"https://github.com/thecomalley/homelab-remote-backup\" target=\"_blank\" rel=\"noopener\"\u003ehomelab-remote-backup\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ehealthchecks.io provides \u0026ldquo;Simple and Effective Cron Job Monitoring\u0026rdquo; via hitting a http url, it can be used as a SaaS tool and is also open-source so you can \u003ca href=\"https://healthchecks.io/docs/self_hosted/\" target=\"_blank\" rel=\"noopener\"\u003eself host it\u003c/a\u003e\u003c/p\u003e","tags":null,"title":"Monitoring cron jobs with healthchecks.io"}]